{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# DAT257x: Reinforcement Learning Explained\n\n## Lab 5: Temporal Difference Learning\n\n### Exercise 5.2: SARSA Agent"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport sys\n\nif \"../\" not in sys.path:\n    sys.path.append(\"../\") \n    \nfrom lib.envs.simple_rooms import SimpleRoomsEnv\nfrom lib.envs.windy_gridworld import WindyGridworldEnv\nfrom lib.envs.cliff_walking import CliffWalkingEnv\nfrom lib.simulation import Experiment",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class Agent(object):  \n        \n    def __init__(self, actions):\n        self.actions = actions\n        self.num_actions = len(actions)\n\n    def act(self, state):\n        raise NotImplementedError",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class SarsaAgent(Agent):\n    \n    def __init__(self, actions, epsilon=0.01, alpha=0.5, gamma=1):\n        super(SarsaAgent, self).__init__(actions)\n        \n        ## TODO 1\n        ## Initialize empty dictionary here\n        ## In addition, initialize the value of epsilon, alpha and gamma\n        self.Q = {}\n        self.eps = epsilon\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def stateToString(self, state):\n        mystring = \"\"\n        if np.isscalar(state):\n            mystring = str(state)\n        else:\n            for digit in state:\n                mystring += str(digit)\n        return mystring    \n    \n    def act(self, state):\n        stateStr = self.stateToString(state)      \n        action = np.random.randint(0, self.num_actions) \n        \n        ## TODO 2\n        ## Implement epsilon greedy policy here\n        \n        return action\n\n    def learn(self, state1, action1, reward, state2, action2):\n        state1Str = self.stateToString(state1)\n        state2Str = self.stateToString(state2)\n        \n        ## TODO 3\n        ## Implement the sarsa update here\n        \n        self.Q[(state1Str,action1)] = self.Q[(state1Str,action1)] + \\\n                          self.alpha*(reward + self.gamma*(self.Q[(state2Str,action2)] - self.Q[(state1Str,action1)]))\n        \n        \"\"\"\n        SARSA Update\n        Q(s,a) <- Q(s,a) + alpha * (reward + gamma * Q(s',a') - Q(s,a))\n        or\n        Q(s,a) <- Q(s,a) + alpha * (td_target - Q(s,a))\n        or\n        Q(s,a) <- Q(s,a) + alpha * td_delta\n        \"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "interactive = True\n%matplotlib nbagg\nenv = SimpleRoomsEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(10, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "interactive = False\n%matplotlib inline\nenv = SimpleRoomsEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(50, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "interactive = True\n%matplotlib nbagg\nenv = CliffWalkingEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(10, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "interactive = False\n%matplotlib inline\nenv = CliffWalkingEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(100, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "interactive = False\n%matplotlib inline\nenv = WindyGridworldEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(50, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}